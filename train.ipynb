{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reading = np.load('reading_np.npy')\n",
    "phn = np.load('phone_np.npy')\n",
    "drinking = np.load('drinking_np.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_reading = np.full(50,0)\n",
    "y_phn = np.full(50,1)\n",
    "y_drink = np.full(50,2)\n",
    "Y = np.concatenate((y_reading, y_phn, y_drink), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = Y.shape[0]\n",
    "\n",
    "N_actions = 3\n",
    "N_features = 16380\n",
    "dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_labels = np.zeros((N, N_actions))\n",
    "one_hot_labels[np.arange(N), Y] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack((phn,reading,drinking))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.from_numpy(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([150, 16380])\n"
     ]
    }
   ],
   "source": [
    "ffw_train = train_x.view(train_x.shape[0],-1)\n",
    "print(ffw_train.shape)\n",
    "\n",
    "mean = ffw_train.mean(dim=0)\n",
    "std = ffw_train.std(dim=0)\n",
    "\n",
    "ffw_train = (ffw_train - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffw_train = ffw_train.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=16380, out_features=2048)\n",
      "  (fc2): Linear(in_features=2048, out_features=512)\n",
      "  (fc3): Linear(in_features=512, out_features=3)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(N_features,2048)\n",
    "        self.fc2 = nn.Linear(2048,512)\n",
    "        self.fc3 = nn.Linear(512,N_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = F.tanh(self.fc3(x))\n",
    "        #print(x.shape)\n",
    "        sft = nn.Softmax(dim=1)\n",
    "        x = sft(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr = 0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = Variable(ffw_train).float()\n",
    "labels = Variable(torch.from_numpy(Y))\n",
    "#labels = Variable(torch.from_numpy(one_hot_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 1.1009\n",
      "Epoch [2/100], Loss: 1.0008\n",
      "Epoch [3/100], Loss: 0.9172\n",
      "Epoch [4/100], Loss: 0.8852\n",
      "Epoch [5/100], Loss: 0.8678\n",
      "Epoch [6/100], Loss: 0.8547\n",
      "Epoch [7/100], Loss: 0.8436\n",
      "Epoch [8/100], Loss: 0.8340\n",
      "Epoch [9/100], Loss: 0.8254\n",
      "Epoch [10/100], Loss: 0.8177\n",
      "Epoch [11/100], Loss: 0.8108\n",
      "Epoch [12/100], Loss: 0.8045\n",
      "Epoch [13/100], Loss: 0.7985\n",
      "Epoch [14/100], Loss: 0.7928\n",
      "Epoch [15/100], Loss: 0.7872\n",
      "Epoch [16/100], Loss: 0.7818\n",
      "Epoch [17/100], Loss: 0.7768\n",
      "Epoch [18/100], Loss: 0.7722\n",
      "Epoch [19/100], Loss: 0.7680\n",
      "Epoch [20/100], Loss: 0.7642\n",
      "Epoch [21/100], Loss: 0.7606\n",
      "Epoch [22/100], Loss: 0.7574\n",
      "Epoch [23/100], Loss: 0.7544\n",
      "Epoch [24/100], Loss: 0.7518\n",
      "Epoch [25/100], Loss: 0.7494\n",
      "Epoch [26/100], Loss: 0.7472\n",
      "Epoch [27/100], Loss: 0.7451\n",
      "Epoch [28/100], Loss: 0.7432\n",
      "Epoch [29/100], Loss: 0.7414\n",
      "Epoch [30/100], Loss: 0.7398\n",
      "Epoch [31/100], Loss: 0.7382\n",
      "Epoch [32/100], Loss: 0.7368\n",
      "Epoch [33/100], Loss: 0.7355\n",
      "Epoch [34/100], Loss: 0.7344\n",
      "Epoch [35/100], Loss: 0.7333\n",
      "Epoch [36/100], Loss: 0.7323\n",
      "Epoch [37/100], Loss: 0.7314\n",
      "Epoch [38/100], Loss: 0.7306\n",
      "Epoch [39/100], Loss: 0.7298\n",
      "Epoch [40/100], Loss: 0.7291\n",
      "Epoch [41/100], Loss: 0.7285\n",
      "Epoch [42/100], Loss: 0.7278\n",
      "Epoch [43/100], Loss: 0.7272\n",
      "Epoch [44/100], Loss: 0.7266\n",
      "Epoch [45/100], Loss: 0.7260\n",
      "Epoch [46/100], Loss: 0.7255\n",
      "Epoch [47/100], Loss: 0.7251\n",
      "Epoch [48/100], Loss: 0.7247\n",
      "Epoch [49/100], Loss: 0.7243\n",
      "Epoch [50/100], Loss: 0.7239\n",
      "Epoch [51/100], Loss: 0.7236\n",
      "Epoch [52/100], Loss: 0.7233\n",
      "Epoch [53/100], Loss: 0.7230\n",
      "Epoch [54/100], Loss: 0.7228\n",
      "Epoch [55/100], Loss: 0.7225\n",
      "Epoch [56/100], Loss: 0.7223\n",
      "Epoch [57/100], Loss: 0.7221\n",
      "Epoch [58/100], Loss: 0.7219\n",
      "Epoch [59/100], Loss: 0.7217\n",
      "Epoch [60/100], Loss: 0.7215\n",
      "Epoch [61/100], Loss: 0.7213\n",
      "Epoch [62/100], Loss: 0.7211\n",
      "Epoch [63/100], Loss: 0.7209\n",
      "Epoch [64/100], Loss: 0.7207\n",
      "Epoch [65/100], Loss: 0.7206\n",
      "Epoch [66/100], Loss: 0.7204\n",
      "Epoch [67/100], Loss: 0.7203\n",
      "Epoch [68/100], Loss: 0.7202\n",
      "Epoch [69/100], Loss: 0.7200\n",
      "Epoch [70/100], Loss: 0.7199\n",
      "Epoch [71/100], Loss: 0.7198\n",
      "Epoch [72/100], Loss: 0.7197\n",
      "Epoch [73/100], Loss: 0.7196\n",
      "Epoch [74/100], Loss: 0.7194\n",
      "Epoch [75/100], Loss: 0.7193\n",
      "Epoch [76/100], Loss: 0.7193\n",
      "Epoch [77/100], Loss: 0.7192\n",
      "Epoch [78/100], Loss: 0.7191\n",
      "Epoch [79/100], Loss: 0.7190\n",
      "Epoch [80/100], Loss: 0.7189\n",
      "Epoch [81/100], Loss: 0.7188\n",
      "Epoch [82/100], Loss: 0.7187\n",
      "Epoch [83/100], Loss: 0.7187\n",
      "Epoch [84/100], Loss: 0.7186\n",
      "Epoch [85/100], Loss: 0.7185\n",
      "Epoch [86/100], Loss: 0.7185\n",
      "Epoch [87/100], Loss: 0.7184\n",
      "Epoch [88/100], Loss: 0.7183\n",
      "Epoch [89/100], Loss: 0.7183\n",
      "Epoch [90/100], Loss: 0.7182\n",
      "Epoch [91/100], Loss: 0.7181\n",
      "Epoch [92/100], Loss: 0.7181\n",
      "Epoch [93/100], Loss: 0.7180\n",
      "Epoch [94/100], Loss: 0.7180\n",
      "Epoch [95/100], Loss: 0.7179\n",
      "Epoch [96/100], Loss: 0.7179\n",
      "Epoch [97/100], Loss: 0.7178\n",
      "Epoch [98/100], Loss: 0.7178\n",
      "Epoch [99/100], Loss: 0.7177\n",
      "Epoch [100/100], Loss: 0.7177\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "        # Convert torch tensor to Variable\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()  # zero the gradient buffer\n",
    "        outputs = net(train_x.cuda())\n",
    "        outputs = outputs\n",
    "        loss = criterion(outputs, labels.cuda())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print ('Epoch [%d/%d], Loss: %.4f' %(epoch+1, num_epochs, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(x):\n",
    "    out = net(x)\n",
    "    weight, action = torch.max(out, dim=1)\n",
    "    return action.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(x,labels):\n",
    "    N = labels.shape[0]\n",
    "    predicted = pred(x.cuda())\n",
    "    i = 0\n",
    "    for yhat,y in zip(predicted,labels):\n",
    "        \n",
    "        if yhat == y:\n",
    "            i += 1\n",
    "    print('Accuracy:', i*100/N)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.66666666666667\n"
     ]
    }
   ],
   "source": [
    "accuracy(train_x, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
