{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reading = np.load('reading_np.npy')\n",
    "phn = np.load('phone_np.npy')\n",
    "drinking = np.load('drinking_np.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_reading = np.full(50,0)\n",
    "y_phn = np.full(50,1)\n",
    "y_drink = np.full(50,2)\n",
    "Y = np.concatenate((y_reading, y_phn, y_drink), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = Y.shape[0]\n",
    "\n",
    "N_actions = 3\n",
    "\n",
    "dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_labels = np.zeros((N, N_actions))\n",
    "one_hot_labels[np.arange(N), Y] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack((phn,reading,drinking))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.arange(X.shape[0])\n",
    "np.random.shuffle(s)\n",
    "shuffled_x = X[s]\n",
    "shuffled_y = Y[s]\n",
    "train_x = shuffled_x[0:int(0.8*N)]\n",
    "train_y = shuffled_y[0:int(0.8*N)]\n",
    "test_x = shuffled_x[int(0.8*N):]\n",
    "test_y = shuffled_y[int(0.8*N):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = Variable(torch.from_numpy(train_x)).float()\n",
    "labels = Variable(torch.from_numpy(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120, 273, 20, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionNet(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, label_size, batch_size):\n",
    "        super(ActionNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2label = nn.Linear(SEQ_LENGTH*hidden_dim, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "        print(self.hidden2label)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # the first is the hidden h\n",
    "        # the second is the cell  c\n",
    "        return (autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim)).cuda(),\n",
    "                autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim)).cuda())\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        y  = self.hidden2label(lstm_out.view(BATCH_SIZE,-1))\n",
    "        log_probs = F.softmax(y, dim=1)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=8736, out_features=3)\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 60\n",
    "HIDDEN_DIM = 32\n",
    "SEQ_LENGTH = train_x.shape[1]\n",
    "LABEL_SIZE = 3\n",
    "BATCH_SIZE = 30\n",
    "num_batches = train_x.shape[0]/BATCH_SIZE\n",
    "net = ActionNet(embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM, label_size=LABEL_SIZE, batch_size=BATCH_SIZE)\n",
    "\n",
    "net = net.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr = 0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 4.6218\n",
      "Epoch [2/100], Loss: 5.3488\n",
      "Epoch [3/100], Loss: 11.3763\n",
      "Epoch [4/100], Loss: 5.8903\n",
      "Epoch [5/100], Loss: 3.6253\n",
      "Epoch [6/100], Loss: 3.7103\n",
      "Epoch [7/100], Loss: 3.5257\n",
      "Epoch [8/100], Loss: 2.6290\n",
      "Epoch [9/100], Loss: 2.5299\n",
      "Epoch [10/100], Loss: 2.2584\n",
      "Epoch [11/100], Loss: 1.7973\n",
      "Epoch [12/100], Loss: 1.6697\n",
      "Epoch [13/100], Loss: 1.8761\n",
      "Epoch [14/100], Loss: 2.5706\n",
      "Epoch [15/100], Loss: 2.4260\n",
      "Epoch [16/100], Loss: 2.4997\n",
      "Epoch [17/100], Loss: 3.1274\n",
      "Epoch [18/100], Loss: 1.8525\n",
      "Epoch [19/100], Loss: 1.6573\n",
      "Epoch [20/100], Loss: 1.8858\n",
      "Epoch [21/100], Loss: 1.2382\n",
      "Epoch [22/100], Loss: 1.3006\n",
      "Epoch [23/100], Loss: 1.2258\n",
      "Epoch [24/100], Loss: 1.0344\n",
      "Epoch [25/100], Loss: 0.8760\n",
      "Epoch [26/100], Loss: 0.9636\n",
      "Epoch [27/100], Loss: 0.8715\n",
      "Epoch [28/100], Loss: 0.7726\n",
      "Epoch [29/100], Loss: 0.7453\n",
      "Epoch [30/100], Loss: 0.7555\n",
      "Epoch [31/100], Loss: 0.7134\n",
      "Epoch [32/100], Loss: 0.6805\n",
      "Epoch [33/100], Loss: 0.7292\n",
      "Epoch [34/100], Loss: 0.9030\n",
      "Epoch [35/100], Loss: 1.0533\n",
      "Epoch [36/100], Loss: 0.7291\n",
      "Epoch [37/100], Loss: 0.7240\n",
      "Epoch [38/100], Loss: 1.0396\n",
      "Epoch [39/100], Loss: 1.1262\n",
      "Epoch [40/100], Loss: 1.4028\n",
      "Epoch [41/100], Loss: 0.9459\n",
      "Epoch [42/100], Loss: 0.6751\n",
      "Epoch [43/100], Loss: 0.7714\n",
      "Epoch [44/100], Loss: 0.6203\n",
      "Epoch [45/100], Loss: 0.6445\n",
      "Epoch [46/100], Loss: 0.4822\n",
      "Epoch [47/100], Loss: 0.5844\n",
      "Epoch [48/100], Loss: 0.6805\n",
      "Epoch [49/100], Loss: 0.7174\n",
      "Epoch [50/100], Loss: 0.5500\n",
      "Epoch [51/100], Loss: 0.4836\n",
      "Epoch [52/100], Loss: 0.3771\n",
      "Epoch [53/100], Loss: 0.3588\n",
      "Epoch [54/100], Loss: 0.5835\n",
      "Epoch [55/100], Loss: 1.0026\n",
      "Epoch [56/100], Loss: 1.3219\n",
      "Epoch [57/100], Loss: 1.2475\n",
      "Epoch [58/100], Loss: 2.1367\n",
      "Epoch [59/100], Loss: 1.0018\n",
      "Epoch [60/100], Loss: 1.1462\n",
      "Epoch [61/100], Loss: 1.2888\n",
      "Epoch [62/100], Loss: 1.6626\n",
      "Epoch [63/100], Loss: 1.0021\n",
      "Epoch [64/100], Loss: 0.7225\n",
      "Epoch [65/100], Loss: 0.7181\n",
      "Epoch [66/100], Loss: 0.4945\n",
      "Epoch [67/100], Loss: 0.4550\n",
      "Epoch [68/100], Loss: 0.4097\n",
      "Epoch [69/100], Loss: 0.3259\n",
      "Epoch [70/100], Loss: 0.3352\n",
      "Epoch [71/100], Loss: 0.3289\n",
      "Epoch [72/100], Loss: 0.2623\n",
      "Epoch [73/100], Loss: 0.2224\n",
      "Epoch [74/100], Loss: 0.2290\n",
      "Epoch [75/100], Loss: 0.3931\n",
      "Epoch [76/100], Loss: 1.7259\n",
      "Epoch [77/100], Loss: 4.2485\n",
      "Epoch [78/100], Loss: 1.9771\n",
      "Epoch [79/100], Loss: 1.7605\n",
      "Epoch [80/100], Loss: 1.3051\n",
      "Epoch [81/100], Loss: 1.4879\n",
      "Epoch [82/100], Loss: 1.3345\n",
      "Epoch [83/100], Loss: 0.9351\n",
      "Epoch [84/100], Loss: 1.2512\n",
      "Epoch [85/100], Loss: 1.1443\n",
      "Epoch [86/100], Loss: 1.0437\n",
      "Epoch [87/100], Loss: 0.6533\n",
      "Epoch [88/100], Loss: 0.5671\n",
      "Epoch [89/100], Loss: 1.0439\n",
      "Epoch [90/100], Loss: 0.6465\n",
      "Epoch [91/100], Loss: 0.4175\n",
      "Epoch [92/100], Loss: 0.8162\n",
      "Epoch [93/100], Loss: 1.4680\n",
      "Epoch [94/100], Loss: 0.9529\n",
      "Epoch [95/100], Loss: 0.7445\n",
      "Epoch [96/100], Loss: 0.9159\n",
      "Epoch [97/100], Loss: 0.7140\n",
      "Epoch [98/100], Loss: 0.6755\n",
      "Epoch [99/100], Loss: 0.7348\n",
      "Epoch [100/100], Loss: 0.6426\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    for batch in range(int(num_batches)):\n",
    "        x = train_x[BATCH_SIZE*batch:BATCH_SIZE*(batch+1)].view(273,BATCH_SIZE,-1).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        net.hidden = net.init_hidden()\n",
    "        \n",
    "        out = net(x)\n",
    "        #print(out.shape)\n",
    "        loss = criterion(out,labels[BATCH_SIZE*batch:BATCH_SIZE*(batch+1)].cuda())\n",
    "        running_loss += loss.data[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print ('Epoch [%d/%d], Loss: %.4f' %(epoch+1, num_epochs, running_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(x):\n",
    "    out = net(x)\n",
    "    weight, action = torch.max(out, dim=1)\n",
    "    return action.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(x,labels):\n",
    "    N = labels.shape[0]\n",
    "    predicted = pred(x.cuda())\n",
    "    i = 0\n",
    "    for yhat,y in zip(predicted,labels):\n",
    "        \n",
    "        if yhat == y:\n",
    "            i += 1\n",
    "    print('Accuracy:', i*100/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = torch.from_numpy(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 273, 20, 3])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(test_x.shape)\n",
    "test_x = test_x.view(SEQ_LENGTH,30,EMBEDDING_DIM)\n",
    "test_x = Variable(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 2, 2, 2, 1, 1, 1, 0, 2, 0, 2, 2, 2, 0, 1, 2, 2, 2, 2,\n",
       "       0, 2, 1, 1, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred(test_x.float().cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.66666666666667\n"
     ]
    }
   ],
   "source": [
    "accuracy(test_x.float().cuda(), test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
