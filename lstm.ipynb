{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "reading = np.load('reading_np.npy')\n",
    "phn = np.load('phone_np.npy')\n",
    "drinking = np.load('drinking_np.npy')\n",
    "cleaning = np.load('cleaning_np.npy')\n",
    "walking = np.load('walking_np.npy')\n",
    "cutting = np.load('cutting_np.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_reading = np.full(50,0)\n",
    "y_phn = np.full(50,1)\n",
    "y_drink = np.full(50,2)\n",
    "y_cleaning = np.full(50,3)\n",
    "y_walking = np.full(50,4)\n",
    "y_cutting = np.full(50,5)\n",
    "Y = np.concatenate((y_reading, y_phn, y_drink, y_cleaning, y_walking, y_cutting), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = Y.shape[0]\n",
    "\n",
    "N_actions = 6\n",
    "\n",
    "dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_labels = np.zeros((N, N_actions))\n",
    "one_hot_labels[np.arange(N), Y] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack((phn,reading,drinking,cleaning,walking,cutting))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temp = frameMat\n",
    "\n",
    "nums_skeleton = 20\n",
    "n_frames = 273\n",
    "frameMat_temp = np.zeros((n_frames, n_features))\n",
    "for i in range(n_frames):\n",
    "    Origin = (temp[i, 12:15] + temp[i, 15:18] + temp[i, 18:21]) / 3\n",
    "    for j in range(num_joints):\n",
    "        index = 3*j\n",
    "        frameMat_temp[i, index] = temp[i, index] - Origin[0]\n",
    "        frameMat_temp[i, index + 1] = temp[i, index + 1] - Origin[1]\n",
    "        frameMat_temp[i, index + 2] = temp[i, index + 2] - Origin[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = X.mean(axis= 0)\n",
    "std  = X.std(axis = 0)\n",
    "X = (X - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.arange(X.shape[0])\n",
    "np.random.shuffle(s)\n",
    "shuffled_x = X[s]\n",
    "shuffled_y = Y[s]\n",
    "train_x = shuffled_x[0:int(0.8*N)]\n",
    "train_y = shuffled_y[0:int(0.8*N)]\n",
    "test_x = shuffled_x[int(0.8*N):]\n",
    "test_y = shuffled_y[int(0.8*N):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = Variable(torch.from_numpy(train_x)).float()\n",
    "test_x = Variable(torch.from_numpy(test_x))\n",
    "test_x = test_x.float().cuda()\n",
    "labels = Variable(torch.from_numpy(train_y))\n",
    "test_y = Variable(torch.from_numpy(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([240, 273, 20, 3])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 273, 20, 3])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionNet(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, label_size, batch_size):\n",
    "        super(ActionNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2label = nn.Linear(SEQ_LENGTH*hidden_dim, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # the first is the hidden h\n",
    "        # the second is the cell  c\n",
    "        return (autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim)).cuda(),\n",
    "                autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim)).cuda())\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        y  = self.hidden2label(lstm_out.view(BATCH_SIZE,-1))\n",
    "        log_probs = F.softmax(y, dim=1)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 60\n",
    "HIDDEN_DIM = 32\n",
    "SEQ_LENGTH = train_x.shape[1]\n",
    "LABEL_SIZE = N_actions\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "num_batches = train_x.shape[0]/BATCH_SIZE\n",
    "num_test_batches = test_x.shape[0]/BATCH_SIZE\n",
    "net = ActionNet(embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM, label_size=LABEL_SIZE, batch_size=BATCH_SIZE)\n",
    "\n",
    "net = net.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load(\"action_net.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel-4.8.1-py3.5.egg/ipykernel_launcher.py:19: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 22.2502, Test Loss: 5.6469\n",
      "Epoch [2/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [3/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [4/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [5/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [6/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [7/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [8/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [9/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [10/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [11/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [12/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [13/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [14/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [15/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [16/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [17/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [18/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [19/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [20/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [21/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [22/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [23/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [24/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [25/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [26/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [27/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [28/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [29/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [30/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [31/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [32/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [33/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [34/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [35/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [36/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [37/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [38/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [39/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [40/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [41/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [42/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [43/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [44/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [45/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [46/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [47/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [48/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [49/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [50/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [51/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [52/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [53/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [54/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [55/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [56/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [57/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [58/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [59/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [60/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [61/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [62/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [63/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [64/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [65/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [66/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [67/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [68/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [69/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [70/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [71/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [72/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [73/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [74/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [75/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [76/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [77/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [78/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [79/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [80/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [81/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [82/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [83/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [84/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [85/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [86/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [87/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [88/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [89/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [90/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [91/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [92/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [93/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [94/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [95/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [96/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [97/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [98/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [99/100], Loss: 22.2502, Test Loss: 5.6261\n",
      "Epoch [100/100], Loss: 22.2502, Test Loss: 5.6261\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    test_running_loss = 0\n",
    "    for batch in range(int(num_test_batches)):\n",
    "        x = test_x[BATCH_SIZE*batch:BATCH_SIZE*(batch+1)].permute(1,0,2,3).contiguous()\n",
    "        x = x.view(SEQ_LENGTH, BATCH_SIZE, -1)\n",
    "        out = net(x)\n",
    "        loss = criterion(out,test_y[BATCH_SIZE*batch:BATCH_SIZE*(batch+1)].cuda())\n",
    "        test_running_loss += loss.data[0]\n",
    "    \n",
    "    test_loss_list.append(test_running_loss)\n",
    "    for batch in range(int(num_batches)):\n",
    "        x = train_x[BATCH_SIZE*batch:BATCH_SIZE*(batch+1)].permute(1,0,2,3).contiguous().cuda()\n",
    "        x = x.view(SEQ_LENGTH, BATCH_SIZE, -1)\n",
    "        optimizer.zero_grad()\n",
    "        net.hidden = net.init_hidden()\n",
    "        \n",
    "        out = net(x)\n",
    "        #print(out.shape)\n",
    "        loss = criterion(out,labels[BATCH_SIZE*batch:BATCH_SIZE*(batch+1)].cuda())\n",
    "        running_loss += loss.data[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss_list.append(running_loss)\n",
    "        \n",
    "    print ('Epoch [%d/%d], Loss: %.4f, Test Loss: %.4f' %(epoch+1, num_epochs, running_loss, test_running_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotGraph(y1,y2,x):\n",
    "    plt.plot(x,y1, label = \"Train loss\")\n",
    "    plt.plot(x,y2, label=\"Test Loss\")\n",
    "    plt.legend()\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotGraph(train_loss_list, test_loss_list, [i for i in range(num_epochs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(x):\n",
    "    out = net(x)\n",
    "    weight, action = torch.max(out, dim=1)\n",
    "    return action.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(x,labels):\n",
    "    N = labels.shape[0]\n",
    "    predicted = pred(x.cuda())\n",
    "    i = 0\n",
    "    for yhat,y in zip(predicted,labels):\n",
    "        \n",
    "        if yhat == y:\n",
    "            i += 1\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([240, 273, 20, 3])\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 273, 20, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Accuracy(x,y):\n",
    "    x = x.float().cuda()\n",
    "    n_testbatch = int(x.shape[0]/BATCH_SIZE)\n",
    "    n_correct = 0\n",
    "    for i in range(n_testbatch):\n",
    "        test_batch_x = x[i*BATCH_SIZE:(i+1)*BATCH_SIZE].view(-1,SEQ_LENGTH, BATCH_SIZE, EMBEDDING_DIM)\n",
    "        print(test_batch_x[0].shape)\n",
    "        n_correct += accuracy(test_batch_x[0], y[i*BATCH_SIZE:(i+1)*BATCH_SIZE])\n",
    "    print(\"Accuracy :\", n_correct*100/x.shape[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([273, 30, 60])\n",
      "torch.Size([273, 30, 60])\n",
      "Accuracy : 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel-4.8.1-py3.5.egg/ipykernel_launcher.py:19: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    }
   ],
   "source": [
    "get_Accuracy(test_x,test_y.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 98.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel-4.8.1-py3.5.egg/ipykernel_launcher.py:19: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    }
   ],
   "source": [
    "get_Accuracy(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type ActionNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(net,\"action_net.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
